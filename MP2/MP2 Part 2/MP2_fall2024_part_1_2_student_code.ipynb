{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbNvxcK6cflg"
      },
      "source": [
        "# Part 1: LLM Setup, Prompting, and Embedding Generations\n",
        "\n",
        "In this part you will learn to setup a Hugging Face LLM, learning to prompt and parse the response, and then finally generate embeddings using an embedding model.\n",
        "In addition, you will vary the temperature parameter to see how that affects the LLM's response.\n",
        "\n",
        "**You will need to write functions to**\n",
        "1. Generate prompts and input them to the tokenizer to the LLM.\n",
        "2. Decode tokenizer's output by the LLM, and parse the LLM response as a string.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opiRRL73v80r"
      },
      "source": [
        "# Task 0: Setting up Environment and the LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcoTVWytbVaV"
      },
      "source": [
        "## Installing necessary packages\n",
        "First, we install Hugging Face and Llama Index libraries that would be used through out the project.\n",
        "\n",
        " **Make sure to run this cell again after a change of runtime type or when a runtime session is terminated.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZTAFBing9JR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S56dEc8gbMMP"
      },
      "outputs": [],
      "source": [
        "# install llama_index\n",
        "! pip install llama_index==0.10.19 llama_index_core==0.10.19 torch llama-index-embeddings-huggingface peft optimum bitsandbytes\n",
        "\n",
        "# install auto_gptq\n",
        "!pip install auto_gptq\n",
        "\n",
        "# install docx2txt\n",
        "! pip install docx2txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeKnBsA3v80s"
      },
      "source": [
        "## LLM Setup\n",
        "Run the following cells below to import necessary libraries, setting up the Hugging Face LLM and prompting the LLM to get an response. In this MP we use the Qwen2.5 (千问) LLM from Alibaba Cloud."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlJEueQjbSHo"
      },
      "outputs": [],
      "source": [
        "# import list of libraries\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings,SimpleDirectoryReader, VectorStoreIndex\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsYssKyNdYoG"
      },
      "outputs": [],
      "source": [
        "# instantiate the LLM from the Hugging Face library\n",
        "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             trust_remote_code=False,\n",
        "                                             revision=\"main\",\n",
        "                                             device_map=\"cuda:0\"\n",
        "                                             )\n",
        "\n",
        "\n",
        "# instantiate the tokenizer, notice that the model name is the same as LLM\n",
        "# why do you think is that, feel free to search online.\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLzA6ZLPv80t"
      },
      "source": [
        "Ensure that the above steps complete without any errors. Once successful, the tokenizer and LLM will be loaded and ready to use\n",
        "\n",
        "**For all steps below, remove ```raise NotImplementedError``` after you have completed the TODO items.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA8ZXdnVv80t"
      },
      "source": [
        "#  Task 1 - LLM Prompting and Output Response Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAKvoYOwftnh"
      },
      "source": [
        "\n",
        "\n",
        "### Step 1: Prompt Generation\n",
        "\n",
        "In this step we write a funciton that can generate the prompts used for prompting the LLM. Populate the prompt generating functions below based on the following specs.\n",
        "\n",
        "**Input:** User query and context\n",
        "\n",
        "**Output:** Prompt as a string.\n",
        "\n",
        "**Example:**\n",
        "1. Context: A customer is having issues with their smartphone battery draining quickly and the phone overheating.\n",
        "2. User query: My battery is constantly draining, what are some suggestions.\n",
        "3. Example output prompt:\n",
        "```\n",
        "Context: A customer is having issues with their smartphone battery draining quickly and the phone overheating.\n",
        "Please respond to the following user comment. Use the context above if it is helpful.\n",
        "User comments: My battery is constantly draining, what are some suggestions.\n",
        "```\n",
        "4. Expected format:\n",
        "```\n",
        "Context:<context>\n",
        "Please respond to the following user comment. Use the context above if it is helpful.\n",
        "User comments: <user query>\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-J3uoTKegQi"
      },
      "outputs": [],
      "source": [
        "# TODO 1: Populate the user prompt generating function\n",
        "def prompt_with_context(context, user_query):\n",
        "  prompt = \"\"\"\n",
        "  TODO: Populate the prompt with context and user query\n",
        "  \"\"\"\n",
        "  raise NotImplementedError\n",
        "  return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTNVJ2sxYfyb"
      },
      "source": [
        "### Step 2: LLM Query Function\n",
        "\n",
        "In this step we write a function to query the LLM given the context and the user_query, using the prompt generation function that you have implemented above.\n",
        "\n",
        "Recall that LLM works with tokens instead of strings and characters directly, as such we need to tokenize the prompt first using tokenizer and decode the tokens generated by the LLM.\n",
        "\n",
        "**Input:** User query and context\n",
        "\n",
        "**Output:** LLM response as a string\n",
        "\n",
        "Refer to the [Hugging Face Tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.encode_plus) for additional information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ag7bW7aMWrIL"
      },
      "outputs": [],
      "source": [
        "def get_llm_response(context, user_query, temperature=0.0001):\n",
        "  # generate the prompt\n",
        "  prompt = prompt_with_context(context, user_query)\n",
        "\n",
        "  # setting up prompting messages, at a high-level\n",
        "  # the overall prompting message sets the role for the LLM (system)\n",
        "  # in this case LLM answers user query, and then also provide the actual\n",
        "  # query contents and usesr query.\n",
        "  # DO NOT change this part!\n",
        "  messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "  ]\n",
        "  input_text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        "  )\n",
        "  # TODO 2: call the tokenizer to tokenize the input_text into tokens\n",
        "  # you can refer to: https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.encode_plus\n",
        "  # for additional information of how to use a tokenizer\n",
        "  # you can also take a look at what texts are tokenized\n",
        "  model_inputs = \"TODO tokenize the input_text\"\n",
        "  raise NotImplementedError\n",
        "\n",
        "  # calling the LLM with input tokens to generate tokenized outputs\n",
        "  generated_token_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=512,\n",
        "    temperature=temperature\n",
        "  )\n",
        "\n",
        "  # TODO 3: post process the generated tokens\n",
        "  # print the generated_token_ids and the model_inputs.input_ids, you will see that\n",
        "  # the first part of the output: generated_token_ids, returned by the LLM contains\n",
        "  # also the input tokens. Post processing step should remove input tokens and\n",
        "  # leaves only the tokens output by the LLM.\n",
        "\n",
        "  def post_processing(generated_token_ids):\n",
        "    # TODO 3: insert post processing logic here to remove the input tokens from the\n",
        "    # generated tokens\n",
        "    raise NotImplementedError\n",
        "    return generated_token_ids\n",
        "\n",
        "  generated_token_ids_post_processed = post_processing(generated_token_ids)\n",
        "\n",
        "  # TODO 4: decode the generated (post_processed) tokens. Hint: Take a look at\n",
        "  # the \"batch_decode\" function of the Hugging Face tokenizer\n",
        "  response = \"TODO: decode the generated tokens\"\n",
        "  raise NotImplementedError\n",
        "\n",
        "  return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9az0F5aGg8i"
      },
      "source": [
        "### Step 3: Query the LLM with Different Temperature Settings\n",
        "\n",
        "In this step, we query the LLM using the implemented functions. We are now ready to write the first query to interact with the Qwen 2.5-1.5B LLM.\n",
        "\n",
        "Additionally, we explore the effect of temperature on LLM inference.\n",
        "\n",
        "**Example prompt (empty context):** What is the functionality of an LLM?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V56yVZUrJHHV"
      },
      "outputs": [],
      "source": [
        "# TODO 5: uses the get_llm_response function implemented\n",
        "# and print the LLM's response for the example prompt.\n",
        "# Keep the temperature parameter at 0.0001 in this step.\n",
        "raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKjaIAFiKsMW"
      },
      "source": [
        "The temperature parameter plays a crucial row in the LLM inference. **Find out what is the role of the temperature parameter and try to generate outputs with different temperature settings. What can you conclude?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McIkYlvnKrMH"
      },
      "outputs": [],
      "source": [
        "# TODO 6: change the temperature parameter value and see how that affects the LLM output.\n",
        "# You might want to repeatly generate a few response of the same prompt to see the difference.\n",
        "raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0nkXcBlhrti"
      },
      "source": [
        "### Step 4: Hallucination in LLM\n",
        "This is an open-ended question.\n",
        "\n",
        "**Construct a prompt such that the prompt leads to a hallucination of LLM. Hallucination have different types: factual hallucination, nonsensical hallucination, or contradicted responses.**\n",
        "\n",
        "**You can also vary the temperature parameter. Does increasing the temperature help?**\n",
        "\n",
        "**Does providing some factual context help in reducing the hallucination?**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcq9XVX5crQX"
      },
      "outputs": [],
      "source": [
        "# TODO 7: Come up a prompt that results in hallucination, then see\n",
        "# if changing the temperature or providing factual context help?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3PdM9shycC3"
      },
      "source": [
        "# Task 2 - Understanding Tokenization and Embeddings\n",
        "In this part you learn to generate embeddings for texts and understand the relationship between tokenization and embeddings. Generating embeddings is essential for sentence and document understanding as well as building a RAG database, Part 3 of this MP.   \n",
        "\n",
        "**You need to write functions to**\n",
        "1. Instantiate an open-sourced Hugging Face embedding model.\n",
        "2. Encode the given sentence examples.\n",
        "3. Implement the cosine vector similarity score.\n",
        "4. Compare the embeddings between those examples and a given reference and select the ones that have a similarity score greater than some threshold.\n",
        "\n",
        "## Embedding Model Setup\n",
        "Run the following cells below to setup the embedding model. In this project we use the all-mpnet-base-v2 embedding model. all-mpnet-base-v2 is an embedding model based on Microsoft's mpnet-base models. This model is intented to encode sentences and short paragraphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9T7iXsc_1ZK6"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from llama_index.core import Document, VectorStoreIndex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOli1yP39Ttj"
      },
      "outputs": [],
      "source": [
        "# Load a sentence-transformers model for text embeddings from Hugging Face\n",
        "embed_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vN3k4dDS-aEG"
      },
      "source": [
        "Ensure that the above steps complete without any errors. Once successful, embedding model is loaded and ready to go.\n",
        "\n",
        "\n",
        "**For all steps below, remove ```raise NotImplementedError``` after you have completed the TODO items.**\n",
        "\n",
        "### Step 1: Embedding Generation\n",
        "\n",
        "In this step we write a funciton that can generate the embeddings used for downstream tasks such as RAG. Populate the embedding generating functions below based on the following specs.\n",
        "\n",
        "**Input:** User input text.\n",
        "\n",
        "**Output:** Text embedding using the embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNOxk-Mz5GD_"
      },
      "outputs": [],
      "source": [
        "# TODO 8: Generate embeddings using the embedding model\n",
        "def generate_embeddings(text):\n",
        "  raise NotImplementedError\n",
        "  return embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yso29AEzCSRW"
      },
      "source": [
        "### Step 2: Cosine Similarity Score\n",
        "\n",
        "In this step we write a funciton that ccomputes the similarity of two embeddings. We use cosine similarity.\n",
        "\n",
        "**Input:** Two embeddings as numpy arrays.\n",
        "\n",
        "**Output:** The cosine similarity score between the two embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYm4j2uJCmTx"
      },
      "outputs": [],
      "source": [
        "# TODO 9: Calculate the cosine similarity score for the two embeddings\n",
        "def cosine_similarity_score(src_embedding, tar_embedding):\n",
        "  raise NotImplementedError\n",
        "  return score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xptaAlEDJG7"
      },
      "source": [
        "### Step 3: Comparing Similarity between Sentences.\n",
        "\n",
        "In this step we uses the implemented embedding generation function and the cosine similarity score function to compute the similiarty between pairs of the provided sentences.\n",
        "\n",
        "Here we provide a target sentence and a set of 10 other sentences to compare to as part of the sentences.txt file. **Write a program to calculate the cosine similarity score between each sentence in the sentences.txt file and the target sentence.**\n",
        "\n",
        "**What can you conclude from the similarity scores between the target sentence and each provided sentences? Can you explain what does embedding of a sentence represent?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOdwM8bG4HJB"
      },
      "outputs": [],
      "source": [
        "# TODO 10: Calculate the cosine similarity score between target sentence and\n",
        "# each of the other sentences provided in the sentences.txt file.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CV9XAhnBwA83"
      },
      "source": [
        "# Part 2: Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "In this part you will learn to setup a vector database Retrieval-Augmented Generation (RAG), learning to query and retrieve the top matching context, and use that to prompt the LLM. You will also evaluate the relevance of the LLM response with and without the RAG context in order to understand whether RAG help the LLM construct a better response.\n",
        "\n",
        "**You will need to write functions to**\n",
        "1. Construct a vector database from document embeddings.\n",
        "2. Retrieve information from vector database and constrcut the prompting context for the LLM.\n",
        "3. Get the LLM responses with and without RAG given the original user prompt."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_ZttXwH-U_1"
      },
      "source": [
        "# Task 0: Setting Up the LLM and the Embedding Model\n",
        "\n",
        "Import the following packages to get started on part 2. We will use the same LLM and the embedding model as we did in part 1. We provide the starter code for setting up these components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhleYq1W-YtU"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings,SimpleDirectoryReader, VectorStoreIndex\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xic7tGOJ--4l"
      },
      "outputs": [],
      "source": [
        "# instantiate the Embedding model from the Hugging Face library\n",
        "# Parameters that can be tuned later to test different models and different chunk sizes\n",
        "Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "Settings.llm = None\n",
        "Settings.chunk_size = 512\n",
        "Settings.chunk_overlap = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPJpHKyf-67a"
      },
      "outputs": [],
      "source": [
        "# instantiate the LLM from the Hugging Face library\n",
        "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             trust_remote_code=False,\n",
        "                                             revision=\"main\",\n",
        "                                             device_map=\"cuda:0\"\n",
        "                                             )\n",
        "\n",
        "\n",
        "# instantiate the tokenizer, notice that the model name is the same as LLM\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CusH8Ix29eBz"
      },
      "source": [
        "# Task 1: Construct the Vector Database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Vrhp9yi90DM"
      },
      "source": [
        "### Step 1: Read the documents\n",
        "\n",
        "In this step we write a helper function that reads the document from a directory from the Google colabs runtime filesystem. To access the filesystem, click on the folder icon on the left control pannel. By the default this leads to ```/content``` folder, and any subfolder or files under this folder can be directly accessed by this notebook.\n",
        "\n",
        "1. Upload the provided 4 machine learning related documents to the ```sample_data``` folder.\n",
        "2. Finsih the following function to read these documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TW1tiv4SwASu"
      },
      "outputs": [],
      "source": [
        "def read_documents(folder_path, file_extension=[\".pdf\", \".docx\"],\n",
        "                   chunk_size=512, chunk_overlap = 64):\n",
        "  Settings.chunk_size = chunk_size\n",
        "  Settings.chunk_overlap = chunk_overlap\n",
        "  # TODO 1: invoke the SimpleDirectoryReader class to read the documents under\n",
        "  # \"folder_path\" of certain extensions.\n",
        "  documents = SimpleDirectoryReader(folder_path,\n",
        "                                    required_exts=file_extension).load_data()\n",
        "\n",
        "\n",
        "  # TODO 2: return the readed documents and the number of documents read\n",
        "  return documents, len(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBqbFL1nCjs4"
      },
      "outputs": [],
      "source": [
        "documents, num_documents = read_documents(\"sample_data\")\n",
        "print(f\"Number of documents read: {num_documents}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pp0UmenmCgAP"
      },
      "source": [
        "### Step 2: Constructing the vector database\n",
        "\n",
        "In this step, we write help functions to the construct vector database using the embedding model and the documents we read.\n",
        "\n",
        "1. Use VectorStoreIndex class to setup an in-memory vector database\n",
        "2. Convert the vector database as an retriever for user interaction\n",
        "3. return the vector database and the retriever for later query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACveVD6eCOZH"
      },
      "outputs": [],
      "source": [
        "def build_vector_database(documents, top_k):\n",
        "  # TODO 3: Setting the vector database using VectorStoreIndex class\n",
        "  # refer to https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_guide/\n",
        "  # of how to use this class.\n",
        "  index = \"TODO setup the vector database store index\"\n",
        "  raise NotImplementedError\n",
        "\n",
        "  # TODO 4: Turn the vector database as a retriever for user intteraction.\n",
        "  # refer to https://legacy.ts.llamaindex.ai/api/classes/VectorIndexRetriever\n",
        "  # of how to use this class. Return only the top_k document in the similarity\n",
        "  # search.\n",
        "  raise NotImplementedError\n",
        "  retriever = \"TODO setup the vector database retriever using the index\"\n",
        "  return index, retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tSzBVP4HTYw"
      },
      "outputs": [],
      "source": [
        "TOP_K = 2\n",
        "index, retriever = build_vector_database(documents, top_k=TOP_K)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MQIFVdN_rEy"
      },
      "source": [
        "# Task 2: Query the Vector Database"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hRG_L02sFA6h"
      },
      "source": [
        "### Step 1: Querying the vector database\n",
        "\n",
        "In this step, we write a function that uses the document retriever that accepts the user prompt as input and returns the top-K relevant document chunks, then return these documents as contexts.\n",
        "\n",
        "1. Assemble the query engine by completing the following todos.\n",
        "2. The function accepts a user prompt and then ask the query engine for retrieving the top-K similar documents.\n",
        "3. Return the text chunks of the top-K documents as additional contexts to be used in later prompting.\n",
        "\n",
        "Use the following example query: \\\n",
        " Query: **\"What are the documents about?\"**\n",
        "\n",
        "Does the query engine and the retriever return the relevant context based on this query?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8cV9XHiCfhy"
      },
      "outputs": [],
      "source": [
        "def database_query(query, retriever, top_k, similarity_cutoff=0.1):\n",
        "  # TODO 5: Using the RetrieverQueryEngine to construct a query engine from the\n",
        "  # retriever, refer to https://docs.llamaindex.ai/en/stable/api_reference/query_engine/retriever/#llama_index.core.query_engine.RetrieverQueryEngine\n",
        "  query_engine = \"TODO setup the query engine using the retriever\"\n",
        "  raise NotImplementedError\n",
        "\n",
        "  # TODO 6: get the response object from the query_engine\n",
        "  raise NotImplementedError\n",
        "  response = \"TODO get the response from the query engine\"\n",
        "\n",
        "  # TODO 7: the response from the query engine is not the original document\n",
        "  # chunks as the query engine acts like a mini chatbot for user interaction\n",
        "  # however, the \"source_nodes\" in the response object contains the original\n",
        "  # document chunks. Iterate through the source_nodes object in the\n",
        "  # response nodes and concatenate the top_k source_node texts as contexts\n",
        "  # the concatenation of the chuncks should be separated by linebreak character.\n",
        "  context = \"TODO get the original document context from the query engine response\"\n",
        "  for i in range(top_k):\n",
        "      raise NotImplementedError\n",
        "\n",
        "  return context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCUclRqzEQYT"
      },
      "outputs": [],
      "source": [
        "query = \"What are the documents about?\"\n",
        "context = database_query(query, retriever, TOP_K)\n",
        "print(context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWTejXK2IEk8"
      },
      "source": [
        "### Step 2: Using the contexts as part of the LLM prompting\n",
        "\n",
        "Recall that in part 1 you have written a user prompt generation function that\n",
        "generates LLM prompt using the user provided query and the context. In part 1, the context is left empty. In this part, we will actually use the document chunks retrieved from the vector database retriever as the additional context to the prompt.\n",
        "\n",
        "1. Invoke the prompt_with_context function and print an example prompt with the user query and the retrieved context.\n",
        "2. Invoke the get_llm_response function get an response from the LLM on the following sample query **WITH** and **WITHOUT** the retrieved context: \\\n",
        "Query: **\"What are the documents about?\"**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbbD8RjdKP60"
      },
      "outputs": [],
      "source": [
        "user_query = \"What are the documents about?\"\n",
        "empty_context = \"\"\n",
        "# TODO 8: invoke the prompt_with_context to examine the prompt\n",
        "# TODO 9: invoke the get_llm_response function to get an response from the LLM\n",
        "raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MVOiPZcJxjH"
      },
      "outputs": [],
      "source": [
        "user_query = \"What are the documents about?\"\n",
        "context = database_query(user_query, retriever, TOP_K)\n",
        "# TODO 10: invoke the prompt_with_context to examine the prompt\n",
        "# TODO 11: invoke the get_llm_response function to get an response from the LLM\n",
        "raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSfYOKb7KUHt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.15 (main, Nov 16 2022, 18:07:41) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "6547c541fadfcd6f05ba0b388254863c4ce81dcd0367794b84de825d35954035"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
