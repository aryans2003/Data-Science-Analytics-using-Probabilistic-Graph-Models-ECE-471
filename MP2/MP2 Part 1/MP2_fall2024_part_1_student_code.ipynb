{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbNvxcK6cflg"
      },
      "source": [
        "# Part 1: LLM Setup, Prompting, and Embedding Generations\n",
        "\n",
        "In this part you will learn to setup a Hugging Face LLM, learning to prompt and parse the response, and then finally generate embeddings using an embedding model.\n",
        "In addition, you will vary the temperature parameter to see how that affects the LLM's response.\n",
        "\n",
        "**You will need to write functions to**\n",
        "1. Generate prompts and input them to the tokenizer to the LLM.\n",
        "2. Decode tokenizer's output by the LLM, and parse the LLM response as a string.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 0: Setting up Environment and the LLM"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FcoTVWytbVaV"
      },
      "source": [
        "## Installing necessary packages\n",
        "First, we install Hugging Face and Llama Index libraries that would be used through out the project. \n",
        "\n",
        " **Make sure to run this cell again after a change of runtime type or when a runtime session is terminated.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S56dEc8gbMMP"
      },
      "outputs": [],
      "source": [
        "# install llama_index\n",
        "! pip install llama_index==0.10.19 llama_index_core==0.10.19 torch llama-index-embeddings-huggingface peft optimum bitsandbytes\n",
        "\n",
        "# install auto_gptq\n",
        "!pip install auto_gptq\n",
        "\n",
        "# install docx2txt\n",
        "! pip install docx2txt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LLM Setup\n",
        "Run the following cells below to import necessary libraries, setting up the Hugging Face LLM and prompting the LLM to get an response. In this MP we use the Qwen2.5 (千问) LLM from Alibaba Cloud."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlJEueQjbSHo"
      },
      "outputs": [],
      "source": [
        "# import list of libraries\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings,SimpleDirectoryReader, VectorStoreIndex\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsYssKyNdYoG"
      },
      "outputs": [],
      "source": [
        "# instantiate the LLM from the Hugging Face library\n",
        "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             trust_remote_code=False,\n",
        "                                             revision=\"main\",\n",
        "                                             device_map=\"cuda:0\"\n",
        "                                             )\n",
        "\n",
        "\n",
        "# instantiate the tokenizer, notice that the model name is the same as LLM\n",
        "# why do you think is that, feel free to search online.\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ensure that the above steps complete without any errors. Once successful, the tokenizer and LLM will be loaded and ready to use\n",
        "\n",
        "**For all steps below, remove ```raise NotImplementedError``` after you have completed the TODO items.**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  Task 1 - LLM Prompting and Output Response Processing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iAKvoYOwftnh"
      },
      "source": [
        "\n",
        "\n",
        "### Step 1: Prompt Generation\n",
        "\n",
        "In this step we write a funciton that can generate the prompts used for prompting the LLM. Populate the prompt generating functions below based on the following specs.\n",
        "\n",
        "**Input:** User query and context\n",
        "\n",
        "**Output:** Prompt as a string.\n",
        "\n",
        "**Example:**\n",
        "1. Context: A customer is having issues with their smartphone battery draining quickly and the phone overheating.\n",
        "2. User query: My battery is constantly draining, what are some suggestions.\n",
        "3. Example output prompt:\n",
        "```\n",
        "Context: A customer is having issues with their smartphone battery draining quickly and the phone overheating.\n",
        "Please respond to the following user comment. Use the context above if it is helpful.\n",
        "User comments: My battery is constantly draining, what are some suggestions.\n",
        "```\n",
        "4. Expected format:\n",
        "```\n",
        "Context:<context>\n",
        "Please respond to the following user comment. Use the context above if it is helpful.\n",
        "User comments: <user query>\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-J3uoTKegQi"
      },
      "outputs": [],
      "source": [
        "# TODO 1: Populate the user prompt generating function\n",
        "def prompt_with_context(context, user_query):\n",
        "  prompt = \"\"\"\n",
        "  TODO: Populate the prompt with context and user query\n",
        "  \"\"\"\n",
        "  raise NotImplementedError\n",
        "  return prompt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oTNVJ2sxYfyb"
      },
      "source": [
        "### Step 2: LLM Query Function\n",
        "\n",
        "In this step we write a function to query the LLM given the context and the user_query, using the prompt generation function that you have implemented above.\n",
        "\n",
        "Recall that LLM works with tokens instead of strings and characters directly, as such we need to tokenize the prompt first using tokenizer and decode the tokens generated by the LLM.\n",
        "\n",
        "**Input:** User query and context\n",
        "\n",
        "**Output:** LLM response as a string\n",
        "\n",
        "Refer to the [Hugging Face Tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.encode_plus) for additional information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ag7bW7aMWrIL"
      },
      "outputs": [],
      "source": [
        "def get_llm_response(context, user_query, temperature=0.0001):\n",
        "  # generate the prompt\n",
        "  prompt = prompt_with_context(context, user_query)\n",
        "\n",
        "  # setting up prompting messages, at a high-level\n",
        "  # the overall prompting message sets the role for the LLM (system)\n",
        "  # in this case LLM answers user query, and then also provide the actual\n",
        "  # query contents and usesr query.\n",
        "  # DO NOT change this part!\n",
        "  messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "  ]\n",
        "  input_text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        "  )\n",
        "  # TODO 2: call the tokenizer to tokenize the input_text into tokens\n",
        "  # you can refer to: https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.encode_plus\n",
        "  # for additional information of how to use a tokenizer\n",
        "  # you can also take a look at what texts are tokenized\n",
        "  model_inputs = \"TODO tokenize the input_text\"\n",
        "  raise NotImplementedError\n",
        "\n",
        "  # calling the LLM with input tokens to generate tokenized outputs\n",
        "  generated_token_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=512,\n",
        "    temperature=temperature\n",
        "  )\n",
        "\n",
        "  # TODO 3: post process the generated tokens\n",
        "  # print the generated_token_ids and the model_inputs.input_ids, you will see that\n",
        "  # the first part of the output: generated_token_ids, returned by the LLM contains\n",
        "  # also the input tokens. Post processing step should remove input tokens and\n",
        "  # leaves only the tokens output by the LLM.\n",
        "\n",
        "  def post_processing(generated_token_ids):\n",
        "    # TODO 3: insert post processing logic here to remove the input tokens from the\n",
        "    # generated tokens\n",
        "    raise NotImplementedError\n",
        "    return generated_token_ids\n",
        "\n",
        "  generated_token_ids_post_processed = post_processing(generated_token_ids)\n",
        "\n",
        "  # TODO 4: decode the generated (post_processed) tokens. Hint: Take a look at\n",
        "  # the \"batch_decode\" function of the Hugging Face tokenizer\n",
        "  response = \"TODO: decode the generated tokens\"\n",
        "  raise NotImplementedError\n",
        "\n",
        "  return response"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "z9az0F5aGg8i"
      },
      "source": [
        "### Step 3: Query the LLM with Different Temperature Settings\n",
        "\n",
        "In this step, we query the LLM using the implemented functions. We are now ready to write the first query to interact with the Qwen 2.5-1.5B LLM. \n",
        "\n",
        "Additionally, we explore the effect of temperature on LLM inference. \n",
        "\n",
        "**Example prompt (empty context):** What is the functionality of an LLM?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V56yVZUrJHHV"
      },
      "outputs": [],
      "source": [
        "# TODO 5: uses the get_llm_response function implemented\n",
        "# and print the LLM's response for the example prompt.\n",
        "# Keep the temperature parameter at 0.0001 in this step.\n",
        "raise NotImplementedError"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qKjaIAFiKsMW"
      },
      "source": [
        "The temperature parameter plays a crucial row in the LLM inference. **Find out what is the role of the temperature parameter and try to generate outputs with different temperature settings. What can you conclude?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McIkYlvnKrMH"
      },
      "outputs": [],
      "source": [
        "# TODO 6: change the temperature parameter value and see how that affects the LLM output.\n",
        "# You might want to repeatly generate a few response of the same prompt to see the difference.\n",
        "raise NotImplementedError"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "v0nkXcBlhrti"
      },
      "source": [
        "### Step 4: Hallucination in LLM\n",
        "This is an open-ended question.\n",
        "\n",
        "**Construct a prompt such that the prompt leads to a hallucination of LLM. Hallucination have different types: factual hallucination, nonsensical hallucination, or contradicted responses.**\n",
        "\n",
        "**You can also vary the temperature parameter. Does increasing the temperature help?**\n",
        "\n",
        "**Does providing some factual context help in reducing the hallucination?**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rcq9XVX5crQX"
      },
      "outputs": [],
      "source": [
        "# TODO 7: Come up a prompt that results in hallucination, then see\n",
        "# if changing the temperature or providing factual context help?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_3PdM9shycC3"
      },
      "source": [
        "# Task 2 - Understanding Tokenization and Embeddings\n",
        "In this part you learn to generate embeddings for texts and understand the relationship between tokenization and embeddings. Generating embeddings is essential for sentence and document understanding as well as building a RAG database, Part 3 of this MP.   \n",
        "\n",
        "**You need to write functions to**\n",
        "1. Instantiate an open-sourced Hugging Face embedding model.\n",
        "2. Encode the given sentence examples.\n",
        "3. Implement the cosine vector similarity score.\n",
        "4. Compare the embeddings between those examples and a given reference and select the ones that have a similarity score greater than some threshold.\n",
        "\n",
        "## Embedding Model Setup\n",
        "Run the following cells below to setup the embedding model. In this project we use the all-mpnet-base-v2 embedding model. all-mpnet-base-v2 is an embedding model based on Microsoft's mpnet-base models. This model is intented to encode sentences and short paragraphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9T7iXsc_1ZK6"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from llama_index.core import Document, VectorStoreIndex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOli1yP39Ttj"
      },
      "outputs": [],
      "source": [
        "# Load a sentence-transformers model for text embeddings from Hugging Face\n",
        "embed_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vN3k4dDS-aEG"
      },
      "source": [
        "Ensure that the above steps complete without any errors. Once successful, embedding model is loaded and ready to go.\n",
        "\n",
        "\n",
        "**For all steps below, remove ```raise NotImplementedError``` after you have completed the TODO items.**\n",
        "\n",
        "### Step 1: Embedding Generation\n",
        "\n",
        "In this step we write a funciton that can generate the embeddings used for downstream tasks such as RAG. Populate the embedding generating functions below based on the following specs.\n",
        "\n",
        "**Input:** User input text.\n",
        "\n",
        "**Output:** Text embedding using the embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNOxk-Mz5GD_"
      },
      "outputs": [],
      "source": [
        "# TODO 8: Generate embeddings using the embedding model\n",
        "def generate_embeddings(text):\n",
        "  raise NotImplementedError\n",
        "  return embedding"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Yso29AEzCSRW"
      },
      "source": [
        "### Step 2: Cosine Similarity Score\n",
        "\n",
        "In this step we write a funciton that ccomputes the similarity of two embeddings. We use cosine similarity.\n",
        "\n",
        "**Input:** Two embeddings as numpy arrays.\n",
        "\n",
        "**Output:** The cosine similarity score between the two embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYm4j2uJCmTx"
      },
      "outputs": [],
      "source": [
        "# TODO 9: Calculate the cosine similarity score for the two embeddings\n",
        "def cosine_similarity_score(src_embedding, tar_embedding):\n",
        "  raise NotImplementedError\n",
        "  return score"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-xptaAlEDJG7"
      },
      "source": [
        "### Step 3: Comparing Similarity between Sentences.\n",
        "\n",
        "In this step we uses the implemented embedding generation function and the cosine similarity score function to compute the similiarty between pairs of the provided sentences.\n",
        "\n",
        "Here we provide a target sentence and a set of 10 other sentences to compare to as part of the sentences.txt file. **Write a program to calculate the cosine similarity score between each sentence in the sentences.txt file and the target sentence.**\n",
        "\n",
        "**What can you conclude from the similarity scores between the target sentence and each provided sentences? Can you explain what does embedding of a sentence represent?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOdwM8bG4HJB"
      },
      "outputs": [],
      "source": [
        "# TODO 10: Calculate the cosine similarity score between target sentence and\n",
        "# each of the other sentences provided in the sentences.txt file.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
